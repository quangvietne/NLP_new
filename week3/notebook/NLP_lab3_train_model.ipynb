{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t35ANokl_r1",
        "outputId": "c1ee61f0-957a-4098-f890-75c7d263886a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark session initialized\n",
            "Đã đọc dữ liệu JSON\n",
            "Tổng số dòng: 30000\n",
            "Đã làm sạch & tách từ\n",
            "Đang huấn luyện mô hình Word2Vec...\n",
            "Huấn luyện xong\n",
            "\n",
            " Các từ tương tự với 'computer':\n",
            "+---------+------------------+\n",
            "|word     |similarity        |\n",
            "+---------+------------------+\n",
            "|uwowned  |0.7178223729133606|\n",
            "|desktop  |0.7055313587188721|\n",
            "|pc       |0.623419463634491 |\n",
            "|computers|0.6192078590393066|\n",
            "|software |0.6015800833702087|\n",
            "+---------+------------------+\n",
            "\n",
            "Spark session stopped\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import Tokenizer, Word2Vec\n",
        "from pyspark.sql.functions import col, lower, regexp_replace, split\n",
        "\n",
        "def main():\n",
        "    # Tạo Spark Session\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"Spark Word2Vec Demo\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    print(\"Spark session initialized\")\n",
        "\n",
        "    #  Đọc dữ liệu JSON (C4 dataset)\n",
        "\n",
        "    data_path = \"/content/c4-train.00000-of-01024-30K.json\"\n",
        "    try:\n",
        "        df = spark.read.json(data_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Lỗi đọc file: {e}\")\n",
        "        spark.stop()\n",
        "        return\n",
        "\n",
        "    print(\"Đã đọc dữ liệu JSON\")\n",
        "    print(f\"Tổng số dòng: {df.count()}\")\n",
        "\n",
        "    # Tiền xử lý văn bản\n",
        "    # Giữ cột 'text', đưa về lowercase, bỏ ký tự đặc biệt, rồi tách từ\n",
        "    df_clean = df.select(lower(col(\"text\")).alias(\"text\"))\n",
        "    df_clean = df_clean.withColumn(\n",
        "        \"text\",\n",
        "        regexp_replace(col(\"text\"), r\"[^a-zA-Z\\s]\", \"\")\n",
        "    )\n",
        "    df_clean = df_clean.withColumn(\n",
        "        \"words\",\n",
        "        split(col(\"text\"), r\"\\s+\")\n",
        "    )\n",
        "\n",
        "    print(\"Đã làm sạch & tách từ\")\n",
        "\n",
        "    # Cấu hình & huấn luyện Word2Vec\n",
        "    word2vec = Word2Vec(\n",
        "        vectorSize=100,\n",
        "        minCount=5,\n",
        "        inputCol=\"words\",\n",
        "        outputCol=\"word_vectors\"\n",
        "    )\n",
        "\n",
        "    print(\"Đang huấn luyện mô hình Word2Vec...\")\n",
        "    model = word2vec.fit(df_clean)\n",
        "    print(\"Huấn luyện xong\")\n",
        "\n",
        "    # Demo: tìm từ tương tự với “computer”\n",
        "    print(\"\\n Các từ tương tự với 'computer':\")\n",
        "    try:\n",
        "        synonyms = model.findSynonyms(\"computer\", 5)\n",
        "        synonyms.show(truncate=False)\n",
        "    except Exception as e:\n",
        "        print(\"Từ 'computer' không tồn tại trong từ điển hoặc dữ liệu quá nhỏ.\")\n",
        "        print(e)\n",
        "\n",
        "    # Dừng Spark\n",
        "    spark.stop()\n",
        "    print(\"Spark session stopped\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}