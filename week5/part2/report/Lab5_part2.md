
# BÃO CÃO THá»°C HÃ€NH: PHÃ‚N LOáº I VÄ‚N Báº¢N Vá»šI LSTM & WORD EMBEDDINGS

## 1. TÃ³m táº¯t káº¿t quáº£ thá»±c nghiá»‡m

DÆ°á»›i Ä‘Ã¢y lÃ  báº£ng tá»•ng há»£p Ä‘á»™ chÃ­nh xÃ¡c (Accuracy) ghi nháº­n Ä‘Æ°á»£c tá»« 4 mÃ´ hÃ¬nh trong file notebook:

| Task | MÃ´ hÃ¬nh | Accuracy | Nháº­n xÃ©t |
| --- | --- | --- | --- |
| **Task 1** | **TF-IDF + Logistic Regression** | **0.8355** | Káº¿t quáº£ tá»‘t nháº¥t, cháº¡y nhanh vÃ  hiá»‡u quáº£. |
| **Task 2** | Word2Vec (Average) + Dense | 0.3268 | Hiá»‡u quáº£ tháº¥p do máº¥t thÃ´ng tin thá»© tá»± tá»« khi tÃ­nh trung bÃ¬nh. |
| **Task 3** | LSTM + Pre-trained Word2Vec | 0.0967 | MÃ´ hÃ¬nh khÃ´ng há»™i tá»¥ tá»‘t, káº¿t quáº£ ráº¥t tháº¥p. |
| **Task 4** | LSTM + End-to-End Embedding | 0.0177 | Káº¿t quáº£ kÃ©m nháº¥t, gáº§n nhÆ° dá»± Ä‘oÃ¡n ngáº«u nhiÃªn. |



## 2. Chi tiáº¿t triá»ƒn khai tá»«ng Task

### â–¡ Task 1: Baseline Model 1 (TF-IDF + Logistic Regression)

**CÃ¡c bÆ°á»›c triá»ƒn khai:**

1. **Tiá»n xá»­ lÃ½:** Sá»­ dá»¥ng `TfidfVectorizer` (giá»›i háº¡n `max_features=5000`) Ä‘á»ƒ chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh cÃ¡c vector Ä‘áº·c trÆ°ng dá»±a trÃªn táº§n suáº¥t tá»«.
2. **MÃ´ hÃ¬nh:** Sá»­ dá»¥ng `LogisticRegression` vá»›i `max_iter=1000` Ä‘á»ƒ phÃ¢n loáº¡i.
3. **Pipeline:** Káº¿t há»£p Vectorizer vÃ  Model vÃ o má»™t pipeline duy nháº¥t `make_pipeline`.
4. **Huáº¥n luyá»‡n:** Gá»i hÃ m `.fit()` trÃªn táº­p train.

**Káº¿t quáº£:**

* MÃ´ hÃ¬nh Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c **83.55%** trÃªn táº­p test.
* CÃ¡c lá»›p nhÆ° `general_affirm`, `transport_taxi` Ä‘áº¡t F1-score tuyá»‡t Ä‘á»‘i (1.0).

### â–¡ Task 2: Baseline Model 2 (Word2Vec + Dense Layer)

**CÃ¡c bÆ°á»›c triá»ƒn khai:**

1. **Word Embedding:** Huáº¥n luyá»‡n mÃ´ hÃ¬nh Word2Vec tá»« Ä‘áº§u (from scratch) trÃªn táº­p dá»¯ liá»‡u train báº±ng thÆ° viá»‡n `gensim`.
* Tham sá»‘: `vector_size=100`, `window=5`.


2. **Feature Engineering:** Viáº¿t hÃ m `sentence_to_avg_vector` Ä‘á»ƒ chuyá»ƒn má»—i cÃ¢u thÃ nh vector trung bÃ¬nh cá»™ng cá»§a cÃ¡c tá»« trong cÃ¢u Ä‘Ã³.
3. **MÃ´ hÃ¬nh:** XÃ¢y dá»±ng máº¡ng nÆ¡-ron Ä‘Æ¡n giáº£n (Feed Forward) vá»›i Keras:
* Lá»›p áº©n: Dense (128 units, activation='relu') + Dropout (0.5).
* Lá»›p Ä‘áº§u ra: Dense (sá»‘ lá»›p, activation='softmax').


4. **Huáº¥n luyá»‡n:** Cháº¡y 100 epochs.

**Káº¿t quáº£:**

* Äá»™ chÃ­nh xÃ¡c giáº£m máº¡nh xuá»‘ng cÃ²n **32.68%**.
* Viá»‡c láº¥y trung bÃ¬nh cá»™ng cÃ¡c vector tá»« Ä‘Ã£ lÃ m máº¥t Ä‘i ngá»¯ nghÄ©a vá» thá»© tá»± cÃ¢u, khiáº¿n mÃ´ hÃ¬nh khÃ³ phÃ¢n biá»‡t cÃ¡c cÃ¢u lá»‡nh phá»©c táº¡p.

### â–¡ Task 3: LSTM Model with Pre-trained Embeddings

**CÃ¡c bÆ°á»›c triá»ƒn khai:**

1. **Tiá»n xá»­ lÃ½ chuá»—i:**
* Sá»­ dá»¥ng `Tokenizer` Ä‘á»ƒ táº¡o tá»« Ä‘iá»ƒn (vocab).
* Chuyá»ƒn vÄƒn báº£n thÃ nh chuá»—i sá»‘ (`texts_to_sequences`).
* Sá»­ dá»¥ng `pad_sequences` Ä‘á»ƒ cá»‘ Ä‘á»‹nh Ä‘á»™ dÃ i cÃ¢u (`max_len=50`).


2. **Embedding Matrix:** Táº¡o ma tráº­n trá»ng sá»‘ tá»« mÃ´ hÃ¬nh Word2Vec Ä‘Ã£ train á»Ÿ Task 2.
3. **MÃ´ hÃ¬nh:**
* Lá»›p **Embedding**: Khá»Ÿi táº¡o vá»›i weights tá»« Word2Vec, thiáº¿t láº­p `trainable=False` (khÃ´ng huáº¥n luyá»‡n láº¡i weights nÃ y).
* Lá»›p **LSTM**: 128 units, dropout=0.2.
* Lá»›p Output: Dense.


4. **Huáº¥n luyá»‡n:** Sá»­ dá»¥ng `EarlyStopping` Ä‘á»ƒ dá»«ng sá»›m náº¿u khÃ´ng cáº£i thiá»‡n.

**Káº¿t quáº£:**

* Äá»™ chÃ­nh xÃ¡c ráº¥t tháº¥p: **9.67%**.
* **NguyÃªn nhÃ¢n:** Vector Word2Vec tá»± train trÃªn táº­p dá»¯ liá»‡u nhá» (khoáº£ng 9000 cÃ¢u) chÆ°a Ä‘á»§ tá»‘t Ä‘á»ƒ lÃ m Ä‘áº·c trÆ°ng cá»‘ Ä‘á»‹nh (frozen) cho LSTM.

### â–¡ Task 4: LSTM Model with End-to-End Training

**CÃ¡c bÆ°á»›c triá»ƒn khai:**

1. **Tiá»n xá»­ lÃ½:** TÆ°Æ¡ng tá»± Task 3 (Tokenize & Padding).
2. **MÃ´ hÃ¬nh:**
* Lá»›p **Embedding**: Khá»Ÿi táº¡o ngáº«u nhiÃªn, thiáº¿t láº­p `trainable=True` Ä‘á»ƒ mÃ´ hÃ¬nh tá»± há»c vector tá»« trong quÃ¡ trÃ¬nh train.
* Lá»›p **LSTM**: 128 units.
* Lá»›p Output: Dense.


3. **Huáº¥n luyá»‡n:** Cháº¡y 100 epochs vá»›i EarlyStopping.

**Káº¿t quáº£:**

* Äá»™ chÃ­nh xÃ¡c tháº¥p nháº¥t: **1.77%**.
* **NguyÃªn nhÃ¢n:** MÃ´ hÃ¬nh Deep Learning (LSTM) cáº§n lÆ°á»£ng dá»¯ liá»‡u lá»›n Ä‘á»ƒ há»c embedding tá»« Ä‘áº§u. Vá»›i táº­p dá»¯ liá»‡u nhá» (~9000 máº«u) vÃ  nhiá»u lá»›p phÃ¢n loáº¡i (64 lá»›p intent), mÃ´ hÃ¬nh gáº·p khÃ³ khÄƒn trong viá»‡c há»™i tá»¥.

---

## 3. CÃ¡ch cháº¡y code vÃ  ghi log káº¿t quáº£

**CÃ¡ch cháº¡y:**

1. Äáº£m báº£o Ä‘Ã£ cÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n: `pandas`, `numpy`, `sklearn`, `gensim`, `tensorflow`.
2. Chá»‰nh sá»­a Ä‘Æ°á»ng dáº«n file csv (`train.csv`, `val.csv`, `test.csv`) trong cell Ä‘áº§u tiÃªn náº¿u cáº§n.
3. Cháº¡y code trong notebook/lab5_lstm.ipynb
4. Cháº¡y láº§n lÆ°á»£t cÃ¡c cell tá»« trÃªn xuá»‘ng dÆ°á»›i (Run All).







## 4 . Giáº£i thÃ­ch káº¿t quáº£ & KhÃ³ khÄƒn gáº·p pháº£i

Sau khi huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ cáº£ 4 pipeline, Ä‘Ã¢y lÃ  káº¿t quáº£

| Pipeline | F1-score (Macro) | Test Loss (hoáº·c Val\_loss) |
| :--- | :---: | :---: |
| **TF-IDF + Logistic Regression** | **0.8353** | N/A |
| Word2Vec (Avg) + Dense | 0.3032 | \~2.5184 |
| Embedding (Pre-trained) + LSTM | 0.0418 | \~3.3181 |
| Embedding (Scratch) + LSTM | 0.0005 | \~4.1240 |

**PhÃ¢n tÃ­ch nhanh káº¿t quáº£ Ä‘á»‹nh lÆ°á»£ng:**

Káº¿t quáº£ Ä‘á»‹nh lÆ°á»£ng cho tháº¥y má»™t Ä‘iá»u ráº¥t rÃµ rÃ ng:

1.  **MÃ´ hÃ¬nh cá»• Ä‘iá»ƒn (TF-IDF + Logistic Regression) chiáº¿n tháº¯ng Ã¡p Ä‘áº£o** vá»›i F1-score (macro) lÃªn Ä‘áº¿n 83.53%.
2.  **Táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh Deep Learning Ä‘á»u tháº¥t báº¡i tháº£m háº¡i.** MÃ´ hÃ¬nh "Word2Vec (Avg) + Dense" chá»‰ Ä‘áº¡t F1-score 30.32%, trong khi cáº£ hai mÃ´ hÃ¬nh LSTM Ä‘á»u cho káº¿t quáº£ gáº§n nhÆ° báº±ng 0 (F1-score 4.18% vÃ  0.05%).
3.  **LÃ½ do tháº¥t báº¡i (sÆ¡ bá»™):** CÃ³ hai lÃ½ do chÃ­nh.
      * **Embedding (Pre-trained) + LSTM:** MÃ´ hÃ¬nh nÃ y tháº¥t báº¡i vÃ¬ nÃ³ sá»­ dá»¥ng vector Word2Vec (`w2v_model`) do chÃºng ta tá»± huáº¥n luyá»‡n trÃªn 8954 cÃ¢u (quÃ¡ Ã­t). Cháº¥t lÆ°á»£ng embedding nÃ y ráº¥t tháº¥p, vÃ  viá»‡c chÃºng ta "Ä‘Ã³ng bÄƒng" nÃ³ (`trainable=False`) Ä‘Ã£ khiáº¿n mÃ´ hÃ¬nh khÃ´ng thá»ƒ há»c Ä‘Æ°á»£c.
      * **Embedding (Scratch) + LSTM:** MÃ´ hÃ¬nh nÃ y tháº¥t báº¡i vÃ¬ **Ä‘Ã³i dá»¯ liá»‡u (Data Starvation)**. Táº­p dá»¯ liá»‡u 8954 cÃ¢u lÃ  quÃ¡ nhá» Ä‘á»ƒ mÃ´ hÃ¬nh cÃ³ thá»ƒ há»c Ä‘á»“ng thá»i cáº£ vector embedding láº«n quan há»‡ ngá»¯ nghÄ©a cá»§a LSTM.

-----

### ğŸ§  PhÃ¢n tÃ­ch Ä‘á»‹nh tÃ­nh (Táº¡i sao LSTM khÃ´ng hoáº¡t Ä‘á»™ng?)

ÄÃ¢y lÃ  pháº§n quan trá»ng nháº¥t. TÃ´i Ä‘Ã£ cháº¡y dá»± Ä‘oÃ¡n trÃªn 3 cÃ¢u "khÃ³" (láº¥y tá»« cell code cuá»‘i cÃ¹ng) vÃ  so sÃ¡nh vá»›i nhÃ£n tháº­t (True Labels) Ä‘Ã£ Ä‘Æ°á»£c sá»­a láº¡i cho chÃ­nh xÃ¡c.

  * **CÃ¢u 1 (Phá»§ Ä‘á»‹nh):** `"can you remind me to not call my mom"`
      * **NhÃ£n tháº­t:** `reminder_create`
  * **CÃ¢u 2 (Cáº¥u trÃºc "hoáº·c"):** `"is it going to be sunny or rainy tomorrow"`
      * **NhÃ£n tháº­t:** `weather_query`
  * **CÃ¢u 3 (Phá»¥ thuá»™c xa & Phá»§ Ä‘á»‹nh):** `"find a flight from new york to london but not through paris"`
      * **NhÃ£n tháº­t:** `flight_search`

**Báº£ng káº¿t quáº£ dá»± Ä‘oÃ¡n (Tá»« Cell code cuá»‘i cÃ¹ng):**

| MÃ´ hÃ¬nh | CÃ¢u 1 ("...not call...") | CÃ¢u 2 ("...sunny or rainy...") | CÃ¢u 3 ("...but not through...") |
| :--- | :--- | :--- | :--- |
| **NhÃ£n tháº­t** | **`reminder_create`** | **`weather_query`** | **`flight_search`** |
| TF-IDF + LR | `calendar_set` (SaiÂ¹) | **`weather_query` (ÄÃºng)** | `general_negate` (Sai) |
| W2V + Dense | `general_quirky` (Sai) | `qa_maths` (Sai) | `transport_query` (SaiÂ²) |
| LSTM (Pre-trained) | `general_explain` (Sai) | `music_query` (Sai) | `lists_createoradd` (Sai) |
| LSTM (Scratch) | `iot_coffee` (Sai) | `iot_coffee` (Sai) | `iot_coffee` (Sai) |

-----

#### PhÃ¢n tÃ­ch chi tiáº¿t:

1.  **Hiá»‡n tÆ°á»£ng "Sá»¥p Ä‘á»• mÃ´ hÃ¬nh" (Model Collapse) cá»§a LSTM (Scratch):**

      * ÄÃ¢y lÃ  phÃ¡t hiá»‡n rÃµ rÃ ng nháº¥t. MÃ´ hÃ¬nh "LSTM (Scratch)" Ä‘Ã£ sá»¥p Ä‘á»• hoÃ n toÃ n. NÃ³ dá»± Ä‘oÃ¡n **`iot_coffee` cho má»i cÃ¢u**.
      * **Giáº£i thÃ­ch:** Vá»›i má»™t táº­p dá»¯ liá»‡u quÃ¡ nhá» (8954 máº«u), mÃ´ hÃ¬nh cÃ³ quÃ¡ nhiá»u tham sá»‘ (pháº£i há»c cáº£ Embedding vÃ  LSTM) Ä‘Ã£ khÃ´ng thá»ƒ há»™i tá»¥. NÃ³ chá»‰ há»c Ä‘Æ°á»£c cÃ¡ch dá»± Ä‘oÃ¡n má»™t lá»›p duy nháº¥t Ä‘á»ƒ giáº£m thiá»ƒu loss. MÃ´ hÃ¬nh nÃ y hoÃ n toÃ n vÃ´ dá»¥ng.

2.  **Sá»± tháº¥t báº¡i cá»§a LSTM (Pre-trained) do Embedding kÃ©m:**

      * MÃ´ hÃ¬nh nÃ y cÅ©ng tháº¥t báº¡i, dá»± Ä‘oÃ¡n cÃ¡c lá»›p sai má»™t cÃ¡ch ngáº«u nhiÃªn (`general_explain`, `music_query`, `lists_createoradd`).
      * **Giáº£i thÃ­ch:** MÃ´ hÃ¬nh Word2Vec (`w2v_model`) Ä‘Æ°á»£c huáº¥n luyá»‡n á»Ÿ Task 2 chá»‰ dá»±a trÃªn 8954 cÃ¢u. ÄÃ¢y lÃ  má»™t embedding "rÃ¡c". Khi tÃ´i náº¡p nÃ³ vÃ o LSTM vÃ  Ä‘áº·t `trainable=False`, tÃ´i Ä‘Ã£ buá»™c mÃ´ hÃ¬nh LSTM pháº£i há»c ngá»¯ nghÄ©a dá»±a trÃªn cÃ¡c vector Ä‘áº§u vÃ o vÃ´ nghÄ©a. DÃ¹ kiáº¿n trÃºc LSTM cÃ³ máº¡nh máº½ Ä‘áº¿n Ä‘Ã¢u, nÃ³ cÅ©ng khÃ´ng thá»ƒ há»c Ä‘Æ°á»£c gÃ¬ tá»« Ä‘áº§u vÃ o kÃ©m cháº¥t lÆ°á»£ng (Garbage In, Garbage Out).

3.  **CÃ¢u 1 & 2 (Chiáº¿n tháº¯ng cho TF-IDF):**

      * MÃ´ hÃ¬nh **TF-IDF + LR** dá»± Ä‘oÃ¡n **Ä‘Ãºng** CÃ¢u 2 (`weather_query`) vÃ  **sai** CÃ¢u 1.
      * **(SaiÂ¹) - CÃ¢u 1:** NhÃ£n tháº­t lÃ  `reminder_create`. TF-IDF dá»± Ä‘oÃ¡n `calendar_set`. ÄÃ¢y lÃ  má»™t lá»—i **sai nhÆ°ng cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c**. Hai intent `reminder_create` (táº¡o nháº¯c nhá»Ÿ) vÃ  `calendar_set` (Ä‘áº·t lá»‹ch) ráº¥t gáº§n nhau vá» máº·t ngá»¯ nghÄ©a vÃ  tá»« khÃ³a (Ä‘á»u dÃ¹ng "remind"). MÃ´ hÃ¬nh Ä‘Ã£ báº¯t Ä‘Ãºng *chá»§ Ä‘á»* nhÆ°ng sai intent cá»¥ thá»ƒ.
      * **(ÄÃºng) - CÃ¢u 2:** MÃ´ hÃ¬nh báº¯t chÃ­nh xÃ¡c tá»« khÃ³a `"sunny"`, `"rainy"`, `"tomorrow"` Ä‘á»ƒ dá»± Ä‘oÃ¡n `weather_query`. Táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh DL khÃ¡c Ä‘á»u sai hoÃ n toÃ n.

4.  **CÃ¢u 3 (CÃ¢u "khÃ³" nháº¥t: "...but not through paris"):**

      * ÄÃ¢y lÃ  cÃ¢u mÃ  **táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh Ä‘á»u dá»± Ä‘oÃ¡n sai**, nhÆ°ng sai theo nhá»¯ng cÃ¡ch khÃ¡c nhau.
      * **TF-IDF + LR (Sai):** Dá»± Ä‘oÃ¡n `general_negate`. Äiá»u nÃ y cho tháº¥y Ä‘iá»ƒm yáº¿u cá»§a nÃ³: nÃ³ tháº¥y "not" vÃ  bá»‹ nháº§m láº«n, nghÄ© ráº±ng Ã½ Ä‘á»‹nh cá»§a cÃ¢u lÃ  "phá»§ Ä‘á»‹nh má»™t Ä‘iá»u gÃ¬ Ä‘Ã³" thay vÃ¬ "tÃ¬m kiáº¿m thÃ´ng tin".
      * **W2V + Dense (SaiÂ²):** Dá»± Ä‘oÃ¡n `transport_query`. ÄÃ¢y lÃ  má»™t lá»—i **sai nhÆ°ng ráº¥t sÃ¡t**. NhÃ£n tháº­t lÃ  `flight_search` (tÃ¬m chuyáº¿n bay), lÃ  má»™t intent con cá»§a `transport_query` (truy váº¥n váº­n táº£i). Giá»‘ng nhÆ° CÃ¢u 1, mÃ´ hÃ¬nh nÃ y Ä‘Ã£ báº¯t Ä‘Ãºng *chá»§ Ä‘á»* nhÆ°ng sai intent cá»¥ thá»ƒ. Báº±ng cÃ¡ch láº¥y trung bÃ¬nh, nÃ³ Ä‘Ã£ bá» qua váº¿ "but not" vÃ  chá»‰ táº­p trung vÃ o cÃ¡c tá»« khÃ³a "flight", "new york", "london".

**Káº¿t luáº­n tá»« phÃ¢n tÃ­ch Ä‘á»‹nh tÃ­nh:**
KhÃ´ng cÃ³ mÃ´ hÃ¬nh nÃ o hiá»ƒu Ä‘Æ°á»£c cÃ¡c cÃ¢u phá»©c táº¡p. CÃ¡c mÃ´ hÃ¬nh LSTM (vá»‘n Ä‘Æ°á»£c ká»³ vá»ng lÃ m tá»‘t viá»‡c nÃ y) Ä‘Ã£ tháº¥t báº¡i hoÃ n toÃ n do khÃ´ng Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»§ (thiáº¿u dá»¯ liá»‡u, embedding kÃ©m). MÃ´ hÃ¬nh TF-IDF, dÃ¹ "ngÃ¢y thÆ¡", láº¡i lÃ  mÃ´ hÃ¬nh duy nháº¥t dá»± Ä‘oÃ¡n Ä‘Ãºng 1/3 cÃ¢u vÃ  cÃ³ 1/3 cÃ¢u sai "cháº¥p nháº­n Ä‘Æ°á»£c" (sai nhÆ°ng gáº§n Ä‘Ãºng).

-----

### âš–ï¸ Nháº­n xÃ©t chung: Æ¯u vÃ  NhÆ°á»£c Ä‘iá»ƒm

Dá»±a trÃªn cÃ¡c káº¿t quáº£ thá»±c nghiá»‡m cá»§a tÃ´i trong file notebook nÃ y:

| PhÆ°Æ¡ng phÃ¡p | Æ¯u Ä‘iá»ƒm (Pros) | NhÆ°á»£c Ä‘iá»ƒm (Cons) |
| :--- | :--- | :--- |
| **TF-IDF + Logistic Regression** | - **Hiá»‡u quáº£ nháº¥t** (F1 \> 83%).<br>- **Cá»±c ká»³ nhanh** vÃ  Ä‘Æ¡n giáº£n Ä‘á»ƒ huáº¥n luyá»‡n.<br>- Hoáº¡t Ä‘á»™ng tá»‘t nháº¥t trÃªn cÃ¡c táº­p dá»¯ liá»‡u nhá» (nhÆ° 8954 máº«u), lÃ  baseline hoÃ n háº£o. | - **KhÃ´ng hiá»ƒu thá»© tá»± tá»«** (bá»‹ Ä‘Ã¡nh lá»«a bá»Ÿi cÃ¢u "but not" vÃ  dá»± Ä‘oÃ¡n sai `general_negate`).<br>- **KhÃ´ng hiá»ƒu ngá»¯ nghÄ©a sÃ¢u** (nháº§m láº«n giá»¯a `reminder_create` vÃ  `calendar_set`). |
| **Word2Vec (Avg) + Dense** | - Báº¯t Ä‘áº§u náº¯m báº¯t Ä‘Æ°á»£c **ngá»¯ nghÄ©a** (semantic) cá»§a tá»«.<br>- Dá»± Ä‘oÃ¡n sai nhÆ°ng "gáº§n Ä‘Ãºng" á»Ÿ CÃ¢u 3 (báº¯t Ä‘Ãºng chá»§ Ä‘á» `transport`). | - **Máº¥t hoÃ n toÃ n thá»© tá»± tá»«** do láº¥y trung bÃ¬nh. ÄÃ¢y lÃ  má»™t bÆ°á»›c thÃ´ sÆ¡.<br>- Káº¿t quáº£ tá»•ng thá»ƒ ráº¥t tá»‡ (F1 \< 31%), lÃ m máº¥t quÃ¡ nhiá»u thÃ´ng tin. |
| **Embedding (Pre-trained) + LSTM** | - *LÃ½ thuyáº¿t:* Hiá»ƒu Ä‘Æ°á»£c thá»© tá»± tá»«. | - **Tháº¥t báº¡i (F1 \< 5%)**.<br>- **Embedding quÃ¡ tá»‡:** MÃ´ hÃ¬nh Word2Vec tá»± huáº¥n luyá»‡n trÃªn 8954 cÃ¢u lÃ  khÃ´ng Ä‘á»§ cháº¥t lÆ°á»£ng.<br>- **`trainable=False`:** "ÄÃ³ng bÄƒng" má»™t embedding tá»‡ lÃ  má»™t sai láº§m chÃ­ máº¡ng. |
| **Embedding (Scratch) + LSTM** | - *LÃ½ thuyáº¿t:* MÃ´ hÃ¬nh máº¡nh nháº¥t, cÃ³ thá»ƒ há»c embedding dÃ nh riÃªng cho tÃ¡c vá»¥. | - **Tháº¥t báº¡i hoÃ n toÃ n (F1 \~ 0%)**.<br>- **ÄÃ³i dá»¯ liá»‡u:** MÃ´ hÃ¬nh quÃ¡ phá»©c táº¡p so vá»›i 8954 máº«u, dáº«n Ä‘áº¿n "model collapse".<br>- Huáº¥n luyá»‡n cháº­m nháº¥t. |


### âš–ï¸ Káº¿t luáº­n chung:**
Sau khi cháº¡y cáº£ 4 mÃ´ hÃ¬nh, cÃ³ má»™t Ä‘iá»u trá»Ÿ nÃªn ráº¥t rÃµ rÃ ng: Deep Learning khÃ´ng pháº£i lÃ  "viÃªn Ä‘áº¡n báº¡c" cho má»i bÃ i toÃ¡n. Thá»±c táº¿ trong bÃ i lab nÃ y, vá»›i táº­p dá»¯ liá»‡u phÃ¢n loáº¡i vÄƒn báº£n tÆ°Æ¡ng Ä‘á»‘i nhá» (chá»‰ ~9000 máº«u), mÃ´ hÃ¬nh thá»‘ng kÃª cá»• Ä‘iá»ƒn TF-IDF + Logistic Regression Ä‘Ã£ mang láº¡i hiá»‡u quáº£ vÆ°á»£t trá»™i, chiáº¿n tháº¯ng má»™t cÃ¡ch Ã¡p Ä‘áº£o.

Trong khi Ä‘Ã³, cÃ¡c kiáº¿n trÃºc phá»©c táº¡p nhÆ° LSTM, dÃ¹ máº¡nh máº½ vá» lÃ½ thuyáº¿t, láº¡i hoÃ n toÃ n tháº¥t báº¡i. ChÃºng bá»‹ "Ä‘Ã³i" dá»¯ liá»‡u (data starvation) vÃ  khÃ´ng thá»ƒ há»c Ä‘Æ°á»£c cÃ¡c má»‘i liÃªn há»‡ phá»©c táº¡p, Ä‘áº·c biá»‡t lÃ  khi khÃ´ng cÃ³ sá»± há»— trá»£ cá»§a embedding cháº¥t lÆ°á»£ng cao (nhÆ° GloVe hay FastText). Äiá»u nÃ y dáº«n Ä‘áº¿n káº¿t quáº£ tá»‡ hÆ¡n Ä‘Ã¡ng ká»ƒ vÃ  lÃ  má»™t bÃ i há»c kinh Ä‘iá»ƒn vá» viá»‡c lá»±a chá»n mÃ´ hÃ¬nh phÃ¹ há»£p vá»›i quy mÃ´ dá»¯ liá»‡u

## 5.  TÃ i liá»‡u tham kháº£o : 
- Link github tháº§y PhÆ°Æ¡ng post trÃªn lá»›p 
- Gá»£i Ã½ code tá»« Grock
- TÃ i liá»‡u tá»« trang chá»§ tensorflow
- ThÆ° viá»‡n: Scikit-learn (TF-IDF, Logistic Regression), Gensim (Word2Vec), TensorFlow/Keras (LSTM).