

# BÃO CÃO THá»°C HÃ€NH PART 3: PART-OF-SPEECH TAGGING Vá»šI RNN 


### âŠ  CÃ¡c bÆ°á»›c triá»ƒn khai

Äá»ƒ giáº£i quyáº¿t bÃ i toÃ¡n nÃ y, mÃ¬nh Ä‘Ã£ chia quy trÃ¬nh thÃ nh 3 cÃ´ng Ä‘oáº¡n chÃ­nh nhÆ° sau:

**Task 1: Chuáº©n bá»‹ dá»¯ liá»‡u**
Äáº§u tiÃªn, mÃ¬nh viáº¿t hÃ m `load_conllu` Ä‘á»ƒ Ä‘á»c dá»¯ liá»‡u thÃ´. Tá»« file gá»‘c chá»©a ráº¥t nhiá»u trÆ°á»ng thÃ´ng tin, mÃ¬nh chá»‰ lá»c láº¥y cáº·p quan trá»ng nháº¥t lÃ  `(Tá»«, NhÃ£n UPOS)`.
Sau Ä‘Ã³, mÃ¬nh xÃ¢y dá»±ng bá»™ tá»« Ä‘iá»ƒn (Vocabulary):

* **Tá»« vá»±ng:** QuÃ©t toÃ n bá»™ táº­p train, mÃ¬nh thu Ä‘Æ°á»£c **16,656** tá»«. Äá»ƒ xá»­ lÃ½ cÃ¡c tÃ¬nh huá»‘ng thá»±c táº¿, mÃ¬nh thÃªm 2 token Ä‘áº·c biá»‡t lÃ  `<UNK>` (cho tá»« láº¡ chÆ°a gáº·p bao giá») vÃ  `<PAD>` (Ä‘á»ƒ láº¥p Ä‘áº§y cÃ¡c cÃ¢u ngáº¯n).
* **NhÃ£n:** Tá»•ng cá»™ng cÃ³ **18** nhÃ£n (bao gá»“m cáº£ nhÃ£n padding).
* **Dataloader:** VÃ¬ cÃ¡c cÃ¢u dÃ i ngáº¯n khÃ¡c nhau, mÃ¬nh dÃ¹ng hÃ m `collate_fn` káº¿t há»£p `pad_sequence` Ä‘á»ƒ "Ã©p" chÃºng vá» cÃ¹ng Ä‘á»™ dÃ i trong má»™t batch thÃ¬ má»›i Ä‘Æ°a vÃ o GPU tÃ­nh toÃ¡n Ä‘Æ°á»£c.

**Task 2: XÃ¢y dá»±ng mÃ´ hÃ¬nh**
MÃ¬nh tá»± code class `SimpleRNNForTokenClas` chá»© khÃ´ng dÃ¹ng model cÃ³ sáºµn. Kiáº¿n trÃºc khÃ¡ cá»• Ä‘iá»ƒn gá»“m 3 táº§ng:

1. **Embedding:** Chuyá»ƒn index cá»§a tá»« thÃ nh vector 100 chiá»u.
2. **RNN:** ÄÃ¢y lÃ  trÃ¡i tim cá»§a mÃ´ hÃ¬nh, dÃ¹ng Ä‘á»ƒ quÃ©t qua chuá»—i vector. MÃ¬nh Ä‘áº·t kÃ­ch thÆ°á»›c áº©n (hidden dim) lÃ  128.
3. **Linear:** Táº§ng cuá»‘i cÃ¹ng Ä‘á»ƒ chiáº¿u káº¿t quáº£ ra 18 nhÃ£n xÃ¡c suáº¥t.

**Task 3: Huáº¥n luyá»‡n vÃ  ÄÃ¡nh giÃ¡**

* MÃ¬nh dÃ¹ng hÃ m loss `CrossEntropyLoss` (nhá»› cÃ i Ä‘áº·t `ignore_index` Ä‘á»ƒ mÃ´ hÃ¬nh khÃ´ng bá»‹ pháº¡t oan khi dá»± Ä‘oÃ¡n sai á»Ÿ máº¥y chá»— padding).
* Tá»‘i Æ°u hÃ³a báº±ng `Adam` vá»›i learning rate 0.001.
* Cháº¡y liÃªn tá»¥c 10 vÃ²ng (epochs), cá»© há»c xong má»™t vÃ²ng lÃ  mÃ¬nh cho kiá»ƒm tra (evaluate) ngay trÃªn táº­p Dev Ä‘á»ƒ xem tÃ¬nh hÃ¬nh tháº¿ nÃ o.

---

### âŠ  CÃ¡ch cháº¡y code vÃ  ghi log káº¿t quáº£

**CÃ¡ch cháº¡y:**
Má»Ÿ code trong notebook/NLP_pos_tagging.ipynb vÃ  cháº¡y nÃ³ 


**Nháº­t kÃ½ huáº¥n luyá»‡n (Log thá»±c táº¿):**
ÄÃ¢y lÃ  káº¿t quáº£ chi tiáº¿t mÃ¬nh ghi láº¡i Ä‘Æ°á»£c sau 10 epoch cÃ y cuá»‘c:

| Epoch | Train Loss | Train Accuracy | Dev Loss | Dev Accuracy | Nháº­n xÃ©t nhanh |
| --- | --- | --- | --- | --- | --- |
| 1 | 0.3493 | 96.31% | 1.4303 | 87.85% | Khá»Ÿi Ä‘áº§u khÃ¡ á»•n |
| **2** | **0.3027** | **96.84%** | **1.4790** | **88.08%** | **Äá»‰nh cao phong Ä‘á»™ (Best Model)** ğŸ† |
| 3 | 0.2639 | 97.27% | 1.5426 | 87.94% | Báº¯t Ä‘áº§u cÃ³ dáº¥u hiá»‡u tá»¥t dá»‘c |
| 4 | 0.2273 | 97.66% | 1.5930 | 87.87% |  |
| 5 | 0.1955 | 97.99% | 1.6623 | 87.79% |  |
| 6 | 0.1699 | 98.27% | 1.7088 | 87.82% |  |
| 7 | 0.1454 | 98.52% | 1.7773 | 87.87% |  |
| 8 | 0.1251 | 98.72% | 1.8454 | 87.41% | Loss tÄƒng cao quÃ¡ |
| 9 | 0.1080 | 98.90% | 1.9170 | 87.56% |  |
| 10 | 0.0945 | 99.01% | 1.9912 | 87.02% | Há»c váº¹t (Overfitting) náº·ng |

* **Káº¿t luáº­n:** MÃ´ hÃ¬nh tá»‘t nháº¥t lÃ  á»Ÿ **Epoch 2**.
* **Äá»™ chÃ­nh xÃ¡c chá»‘t háº¡ trÃªn táº­p Dev:** **88.08%**.

---

### â€¢ [] Giáº£i thÃ­ch cÃ¡c káº¿t quáº£ thu Ä‘Æ°á»£c

1. **Chuyá»‡n gÃ¬ Ä‘Ã£ xáº£y ra khi train?**
NhÃ¬n vÃ o báº£ng sá»‘ liá»‡u, cÃ³ thá»ƒ tháº¥y mÃ´ hÃ¬nh Simple RNN nÃ y há»c ráº¥t nhanh, ngay epoch 2 Ä‘Ã£ Ä‘áº¡t Ä‘á»‰nh. Tuy nhiÃªn, tá»« epoch 3 trá»Ÿ Ä‘i xáº£y ra hiá»‡n tÆ°á»£ng "há»c váº¹t" (overfitting). Trong khi Ä‘iá»ƒm trÃªn táº­p Train cá»© tÄƒng vÃ¹n vá»¥t (lÃªn tá»›i 99%) thÃ¬ Ä‘iá»ƒm trÃªn táº­p Dev láº¡i Ä‘i xuá»‘ng vÃ  Loss thÃ¬ tÄƒng máº¡nh. Äiá»u nÃ y chá»©ng tá» mÃ´ hÃ¬nh Ä‘ang cá»‘ nhá»› Ä‘Ã¡p Ã¡n cá»§a táº­p train thay vÃ¬ há»c quy luáº­t ngá»¯ phÃ¡p tá»•ng quÃ¡t.
2. **Thá»­ thÃ¡ch vá»›i cÃ¢u má»›i (Task NÃ¢ng cao):**
MÃ¬nh Ä‘Ã£ viáº¿t hÃ m `predict_sentence` Ä‘á»ƒ test thá»­ má»™t cÃ¢u ngoÃ i lá»: *"I love NLP and PyTorch"*.
Káº¿t quáº£ mÃ´ hÃ¬nh tráº£ vá» nhÆ° sau:
* `I` -> `PRON` (ÄÃºng)
* `love` -> `VERB` (ÄÃºng)
* `NLP` -> `PROPN` (ÄÃºng)
* `and` -> `CCONJ` (ÄÃºng)
* `PyTorch` -> **`ADV` (Sai)**.


*Táº¡i sao sai?* Tá»« "PyTorch" cÃ³ láº½ khÃ´ng cÃ³ trong táº­p train, nÃªn nÃ³ bá»‹ quy vá» token láº¡ `<UNK>`. Cá»™ng thÃªm viá»‡c mÃ´ hÃ¬nh RNN Ä‘Æ¡n giáº£n kháº£ nÄƒng nhá»› ngá»¯ cáº£nh xa hÆ¡i kÃ©m, nÃªn nÃ³ Ä‘oÃ¡n bá»«a thÃ nh Tráº¡ng tá»« thay vÃ¬ Danh tá»« riÃªng.

---

### â€¢ [] NÃªu rÃµ cÃ¡c khÃ³ khÄƒn gáº·p pháº£i vÃ  cÃ¡ch giáº£i quyáº¿t

Trong quÃ¡ trÃ¬nh lÃ m cÅ©ng gáº·p vÃ i cÃ¡i "há»‘", cá»¥ thá»ƒ lÃ :

1. **CÃ¢u dÃ i cÃ¢u ngáº¯n khÃ´ng Ä‘á»u:**
* *Váº¥n Ä‘á»:* MÃ¡y tÃ­nh thÃ­ch ma tráº­n vuÃ´ng vá»©c, mÃ  cÃ¢u vÄƒn thÃ¬ Ä‘á»™ dÃ i vÃ´ chá»«ng.
* *Giáº£i quyáº¿t:* MÃ¬nh pháº£i dÃ¹ng `pad_sequence` Ä‘á»ƒ chÃ¨n thÃªm token Ä‘á»‡m `<PAD>` vÃ o Ä‘uÃ´i cÃ¢u ngáº¯n. Quan trá»ng nháº¥t lÃ  lÃºc tÃ­nh Ä‘iá»ƒm sá»‘ (Accuracy/Loss), mÃ¬nh pháº£i báº£o code lá» máº¥y cÃ¡i token nÃ y Ä‘i (`ignore_index`), khÃ´ng thÃ¬ káº¿t quáº£ sai lá»‡ch háº¿t.


2. **Tá»« láº¡ chÆ°a gáº·p bao giá» (OOV):**
* *Váº¥n Ä‘á»:* LÃºc test gáº·p tá»« má»›i toanh thÃ¬ mÃ´ hÃ¬nh bá»‹ lá»—i ngay.
* *Giáº£i quyáº¿t:* MÃ¬nh quy hoáº¡ch táº¥t cáº£ cÃ¡c tá»« láº¡ vá» má»™t má»‘i lÃ  token `<UNK>`. ThÃ  Ä‘oÃ¡n sai (nhÆ° vá»¥ PyTorch á»Ÿ trÃªn) cÃ²n hÆ¡n lÃ  sáº­p chÆ°Æ¡ng trÃ¬nh.


3. **Háº¡n cháº¿ cá»§a kiáº¿n trÃºc RNN cá»• Ä‘iá»ƒn:**
* *Váº¥n Ä‘á»:* Train Ä‘Æ°á»£c tÃ­ xÃ­u lÃ  bá»‹ Overfitting vÃ  khÃ³ há»c Ä‘Æ°á»£c máº¥y cÃ¢u phá»©c táº¡p do váº¥n Ä‘á» biáº¿n máº¥t Ä‘áº¡o hÃ m (vanishing gradient).
* *HÆ°á»›ng giáº£i quyáº¿t:* BÃ i nÃ y yÃªu cáº§u dÃ¹ng RNN thÃ¬ mÃ¬nh dÃ¹ng, chá»© náº¿u muá»‘n xá»‹n hÆ¡n thÃ¬ cháº¯c cháº¯n pháº£i chuyá»ƒn sang LSTM hoáº·c GRU.



---

### â€¢ [] Nguá»“n tham kháº£o

* **Dá»¯ liá»‡u:** MÃ¬nh sá»­ dá»¥ng bá»™ Universal Dependencies v2.5 (English-EWT).
* **Code:** Tham kháº£o tÃ i liá»‡u chÃ­nh chá»§ cá»§a PyTorch vá» `nn.RNN` vÃ  `CrossEntropyLoss`.

---

### â€¢ [] ThÃ´ng tin Model

* **Model:** ÄÃ¢y lÃ  mÃ´ hÃ¬nh mÃ¬nh **tá»± xÃ¢y dá»±ng vÃ  huáº¥n luyá»‡n tá»« Ä‘áº§u (train from scratch)**, hoÃ n toÃ n khÃ´ng sá»­ dá»¥ng báº¥t ká»³ pre-trained model nÃ o nhÆ° BERT hay GPT.
* **Kiáº¿n trÃºc:** Simple RNN thuáº§n tÃºy.s