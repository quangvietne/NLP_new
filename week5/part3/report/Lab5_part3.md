

# BÃO CÃO THá»°C HÃ€NH: GÃN NHÃƒN Tá»ª LOáº I (POS TAGGING) Vá»šI SIMPLE RNN

## I. Tá»•ng quan

BÃ i thá»±c hÃ nh xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh **Recurrent Neural Network (RNN)** cÆ¡ báº£n Ä‘á»ƒ giáº£i quyáº¿t bÃ i toÃ¡n GÃ¡n nhÃ£n tá»« loáº¡i (Part-of-Speech Tagging). MÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ gÃ¡n nhÃ£n ngá»¯ phÃ¡p (nhÆ° Danh tá»«, Äá»™ng tá»«, TÃ­nh tá»«...) cho tá»«ng tá»« trong cÃ¢u tiáº¿ng Anh.

* **Dá»¯ liá»‡u:** Universal Dependencies (UD_English-EWT).
* **MÃ´ hÃ¬nh:** Simple RNN (Custom build).
* **ThÆ° viá»‡n:** PyTorch.

---

## II. CÃ¡c bÆ°á»›c triá»ƒn khai

### 1. Xá»­ lÃ½ dá»¯ liá»‡u (Data Preprocessing)

* **Äá»c dá»¯ liá»‡u CoNLL-U:** Viáº¿t hÃ m `load_conllu` Ä‘á»ƒ trÃ­ch xuáº¥t tá»« (`FORM`) vÃ  nhÃ£n tá»« loáº¡i (`UPOS`) tá»« file dá»¯ liá»‡u thÃ´.
* **XÃ¢y dá»±ng tá»« Ä‘iá»ƒn (Vocabulary):**
* Táº¡o `word_to_ix`: Ãnh xáº¡ tá»« sang chá»‰ sá»‘. Bao gá»“m token Ä‘áº·c biá»‡t `<UNK>` (cho tá»« láº¡) vÃ  `<PAD>` (cho Ä‘á»‡m). KÃ­ch thÆ°á»›c tá»« Ä‘iá»ƒn: **16,656** tá»«.
* Táº¡o `tag_to_ix`: Ãnh xáº¡ nhÃ£n sang chá»‰ sá»‘. KÃ­ch thÆ°á»›c: **18** nhÃ£n (bao gá»“m nhÃ£n Ä‘á»‡m).



### 2. Chuáº©n bá»‹ Pipeline dá»¯ liá»‡u (Dataset & DataLoader)

* **Dataset:** Lá»›p `POSDataset` chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh cÃ¡c tensor chá»‰ sá»‘. Xá»­ lÃ½ tá»« láº¡ báº±ng cÃ¡ch gÃ¡n vá» chá»‰ sá»‘ cá»§a `<UNK>`.
* **Collator & Padding:**
* Sá»­ dá»¥ng hÃ m `collate_fn` vÃ  `pad_sequence` Ä‘á»ƒ xá»­ lÃ½ cÃ¡c cÃ¢u cÃ³ Ä‘á»™ dÃ i khÃ´ng Ä‘á»“ng Ä‘á»u trong má»™t batch.
* CÃ¡c cÃ¢u ngáº¯n Ä‘Æ°á»£c thÃªm token `<PAD>` vÃ o cuá»‘i Ä‘á»ƒ Ä‘áº¡t Ä‘á»™ dÃ i báº±ng cÃ¢u dÃ i nháº¥t trong batch.


* **DataLoader:** Chia dá»¯ liá»‡u thÃ nh cÃ¡c batch kÃ­ch thÆ°á»›c 32 (Batch Size = 32).

### 3. Kiáº¿n trÃºc MÃ´ hÃ¬nh (Model Architecture)

MÃ´ hÃ¬nh `SimpleRNNForTokenClas` bao gá»“m 3 thÃ nh pháº§n chÃ­nh:

1. **Embedding Layer:** KÃ­ch thÆ°á»›c `(Vocab_Size, 100)`. Chuyá»ƒn Ä‘á»•i chá»‰ sá»‘ tá»« thÃ nh vector Ä‘áº·c trÆ°ng 100 chiá»u. Há»— trá»£ tham sá»‘ `padding_idx` Ä‘á»ƒ bá» qua tÃ­nh toÃ¡n cho token Ä‘á»‡m.
2. **RNN Layer:** KÃ­ch thÆ°á»›c áº©n `hidden_dim = 128`. Nháº­n chuá»—i embedding vÃ  tráº£ vá» chuá»—i tráº¡ng thÃ¡i áº©n (hidden states) Ä‘áº¡i diá»‡n cho ngá»¯ cáº£nh cá»§a tá»«ng tá»«.
3. **Linear Layer:** KÃ­ch thÆ°á»›c `(128, 18)`. Ãnh xáº¡ tráº¡ng thÃ¡i áº©n sang xÃ¡c suáº¥t cá»§a 18 nhÃ£n tá»« loáº¡i.

### 4. Thiáº¿t láº­p Huáº¥n luyá»‡n

* **Loss Function:** `CrossEntropyLoss` vá»›i `ignore_index=PAD_TAG_INDEX`. Äiá»u nÃ y cá»±c ká»³ quan trá»ng Ä‘á»ƒ mÃ´ hÃ¬nh khÃ´ng tÃ­nh lá»—i (loss) táº¡i cÃ¡c vá»‹ trÃ­ lÃ  token Ä‘á»‡m `<PAD>`.
* **Optimizer:** `Adam` vá»›i learning rate `0.001`.
* **Training Loop:** Thá»±c hiá»‡n huáº¥n luyá»‡n qua 10 epochs, cÃ³ tÃ­ch há»£p Ä‘Ã¡nh giÃ¡ (Validation) trÃªn táº­p Dev sau má»—i epoch Ä‘á»ƒ theo dÃµi hiá»‡u nÄƒng.

---

## III. CÃ¡ch cháº¡y code vÃ  Ghi log káº¿t quáº£

### 1. CÃ¡ch cháº¡y code

* **MÃ´i trÆ°á»ng:** Python 3.11, PyTorch. CÃ³ thá»ƒ cháº¡y trÃªn Google Colab hoáº·c Local Jupyter Notebook.
* **Thá»±c thi:** Cháº¡y láº§n lÆ°á»£t cÃ¡c cell tá»« trÃªn xuá»‘ng dÆ°á»›i.
* **LÆ°u Ã½:** Cáº§n Ä‘áº£m báº£o Ä‘Æ°á»ng dáº«n file dá»¯ liá»‡u (`.conllu`) chÃ­nh xÃ¡c. Code há»— trá»£ tá»± Ä‘á»™ng chuyá»ƒn sang GPU (`cuda`) Ä‘á»ƒ tÄƒng tá»‘c.

### 2. Káº¿t quáº£ thá»‘ng kÃª tá»« log thá»±c táº¿

DÆ°á»›i Ä‘Ã¢y lÃ  báº£ng sá»‘ liá»‡u Ä‘áº§y Ä‘á»§ Ä‘Æ°á»£c ghi nháº­n tá»« quÃ¡ trÃ¬nh huáº¥n luyá»‡n 10 Epochs trong notebook:

| Epoch | Thá»i gian (s) | Train Loss | Train Accuracy | Dev Loss | Dev Accuracy | Nháº­n xÃ©t |
| --- | --- | --- | --- | --- | --- | --- |
| 1 | 2.20s | 0.3493 | 96.31% | 1.4303 | 87.85% | Khá»Ÿi Ä‘áº§u tá»‘t |
| **2** | **1.83s** | **0.3027** | **96.84%** | **1.4790** | **88.08%** | **Best Dev Accuracy** ğŸ† |
| 3 | 1.81s | 0.2639 | 97.27% | 1.5426 | 87.94% | Dev Acc giáº£m nháº¹ |
| 4 | 1.81s | 0.2273 | 97.66% | 1.5930 | 87.87% |  |
| 5 | 1.83s | 0.1955 | 97.99% | 1.6623 | 87.79% |  |
| 6 | 1.82s | 0.1699 | 98.27% | 1.7088 | 87.82% |  |
| 7 | 1.80s | 0.1454 | 98.52% | 1.7773 | 87.87% |  |
| 8 | 1.82s | 0.1251 | 98.72% | 1.8454 | 87.41% | Dev Loss tÄƒng cao |
| 9 | 1.84s | 0.1080 | 98.90% | 1.9170 | 87.56% |  |
| 10 | 1.80s | 0.0945 | 99.01% | 1.9912 | 87.02% | Overfitting rÃµ rá»‡t |

---

## IV. Giáº£i thÃ­ch cÃ¡c káº¿t quáº£ thu Ä‘Æ°á»£c

### 1. PhÃ¢n tÃ­ch quÃ¡ trÃ¬nh huáº¥n luyá»‡n

* **Äiá»ƒm tá»‘i Æ°u:** MÃ´ hÃ¬nh Ä‘áº¡t hiá»‡u suáº¥t tá»‘t nháº¥t trÃªn táº­p kiá»ƒm thá»­ (Dev) táº¡i **Epoch 2** vá»›i Ä‘á»™ chÃ­nh xÃ¡c **88.08%**.
* **Hiá»‡n tÆ°á»£ng Overfitting:**
* TrÃªn táº­p Train: Loss giáº£m Ä‘á»u Ä‘áº·n (0.34 -> 0.09) vÃ  Accuracy tÄƒng tiá»‡m cáº­n má»©c tuyá»‡t Ä‘á»‘i (96% -> 99%).
* TrÃªn táº­p Dev: Tá»« sau Epoch 2, Accuracy báº¯t Ä‘áº§u chá»¯ng láº¡i vÃ  giáº£m dáº§n (88.08% -> 87.02%), trong khi Dev Loss tÄƒng máº¡nh (1.47 -> 1.99).
* *Káº¿t luáº­n:* MÃ´ hÃ¬nh Simple RNN báº¯t Ä‘áº§u há»c thuá»™c lÃ²ng dá»¯ liá»‡u huáº¥n luyá»‡n thay vÃ¬ tá»•ng quÃ¡t hÃ³a quy luáº­t ngá»¯ phÃ¡p tá»« sau Epoch 2.



### 2. PhÃ¢n tÃ­ch káº¿t quáº£ dá»± Ä‘oÃ¡n (Inference Task)

MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c thá»­ nghiá»‡m vá»›i cÃ¢u: *"I love NLP and PyTorch"*

| Tá»« (Token) | NhÃ£n dá»± Ä‘oÃ¡n | NhÃ£n thá»±c táº¿ (Ká»³ vá»ng) | ÄÃ¡nh giÃ¡ |
| --- | --- | --- | --- |
| **I** | `PRON` (Äáº¡i tá»«) | `PRON` | âœ… ChÃ­nh xÃ¡c |
| **love** | `VERB` (Äá»™ng tá»«) | `VERB` | âœ… ChÃ­nh xÃ¡c |
| **NLP** | `PROPN` (Danh tá»« riÃªng) | `PROPN` | âœ… ChÃ­nh xÃ¡c |
| **and** | `CCONJ` (LiÃªn tá»«) | `CCONJ` | âœ… ChÃ­nh xÃ¡c |
| **PyTorch** | `ADV` (Tráº¡ng tá»«) | `PROPN` | âŒ Sai |

* **Giáº£i thÃ­ch lá»—i:** Tá»« "PyTorch" bá»‹ gÃ¡n nhÃ£n sai thÃ nh `ADV` (Tráº¡ng tá»«).
* *NguyÃªn nhÃ¢n:* "PyTorch" cÃ³ thá»ƒ lÃ  tá»« khÃ´ng cÃ³ trong tá»« Ä‘iá»ƒn (`<UNK>`). MÃ´ hÃ¬nh RNN Ä‘Æ¡n giáº£n cÃ³ thá»ƒ gáº·p khÃ³ khÄƒn khi dá»±a vÃ o ngá»¯ cáº£nh "and ..." Ä‘á»ƒ suy luáº­n ra Ä‘Ã¢y lÃ  má»™t danh tá»« riÃªng, dáº«n Ä‘áº¿n dá»± Ä‘oÃ¡n sai.



---

## V. KhÃ³ khÄƒn gáº·p pháº£i vÃ  CÃ¡ch giáº£i quyáº¿t

### 1. Váº¥n Ä‘á» Ä‘á»™ dÃ i cÃ¢u khÃ´ng Ä‘á»“ng nháº¥t

* **KhÃ³ khÄƒn:** KhÃ´ng thá»ƒ gom cÃ¡c cÃ¢u cÃ³ Ä‘á»™ dÃ i khÃ¡c nhau vÃ o cÃ¹ng má»™t Tensor Ä‘á»ƒ tÃ­nh toÃ¡n song song trÃªn GPU.
* **Giáº£i quyáº¿t:** Sá»­ dá»¥ng ká»¹ thuáº­t **Padding**. ThÃªm cÃ¡c giÃ¡ trá»‹ `0` (hoáº·c index cá»§a `<PAD>`) vÃ o cuá»‘i cÃ¢u ngáº¯n. Khi tÃ­nh Loss, sá»­ dá»¥ng tham sá»‘ `ignore_index` Ä‘á»ƒ bá» qua cÃ¡c vá»‹ trÃ­ nÃ y.

### 2. Tá»« vá»±ng chÆ°a biáº¿t (Out-Of-Vocabulary - OOV)

* **KhÃ³ khÄƒn:** Khi gáº·p cÃ¡c tá»« má»›i (vÃ­ dá»¥ tÃªn riÃªng, thuáº­t ngá»¯ má»›i nhÆ° "PyTorch") trong táº­p test, mÃ´ hÃ¬nh sáº½ bá»‹ lá»—i náº¿u khÃ´ng cÃ³ cÆ¡ cháº¿ xá»­ lÃ½.
* **Giáº£i quyáº¿t:** XÃ¢y dá»±ng token Ä‘áº·c biá»‡t `<UNK>` trong tá»« Ä‘iá»ƒn. Má»i tá»« khÃ´ng tÃ¬m tháº¥y trong `word_to_ix` sáº½ Ä‘Æ°á»£c Ã¡nh xáº¡ vá» index cá»§a `<UNK>`.

### 3. Háº¡n cháº¿ cá»§a Simple RNN

* **KhÃ³ khÄƒn:** Simple RNN gáº·p váº¥n Ä‘á» **Vanishing Gradient** (biáº¿n máº¥t Ä‘áº¡o hÃ m), khiáº¿n nÃ³ khÃ³ há»c Ä‘Æ°á»£c cÃ¡c phá»¥ thuá»™c xa trong cÃ¢u dÃ i (vÃ­ dá»¥: chá»§ ngá»¯ á»Ÿ Ä‘áº§u cÃ¢u áº£nh hÆ°á»Ÿng Ä‘áº¿n Ä‘á»™ng tá»« á»Ÿ cuá»‘i cÃ¢u).
* **Giáº£i quyáº¿t (Äá»‹nh hÆ°á»›ng):** Trong cÃ¡c bÃ i nÃ¢ng cao, nÃªn thay tháº¿ `nn.RNN` báº±ng `nn.LSTM` (Long Short-Term Memory) hoáº·c `nn.GRU` Ä‘á»ƒ cáº£i thiá»‡n kháº£ nÄƒng ghi nhá»› dÃ i háº¡n vÃ  tÄƒng Ä‘á»™ chÃ­nh xÃ¡c.

---

## VI. ThÃ´ng tin Model vÃ  Nguá»“n tham kháº£o

### 1. ThÃ´ng tin Model (Custom Build)

MÃ´ hÃ¬nh Ä‘Æ°á»£c xÃ¢y dá»±ng tá»« Ä‘áº§u (Train from scratch), khÃ´ng sá»­ dá»¥ng pre-trained weights.

* **Loáº¡i mÃ´ hÃ¬nh:** Token Classification (Sequence Labeling).
* **Kiáº¿n trÃºc:** Embedding (100) -> Simple RNN (128) -> Linear -> Softmax.
* **Prompt/Input:** CÃ¢u vÄƒn báº£n tiáº¿ng Anh Ä‘Æ°á»£c tÃ¡ch tá»« (tokenized).

### 2. Nguá»“n tham kháº£o

* **Dá»¯ liá»‡u:** Universal Dependencies (UD) - English EWT (English Web Treebank).
* **TÃ i liá»‡u ká»¹ thuáº­t:**
* [PyTorch Documentation - RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)
* [PyTorch Documentation - Padding Sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html)