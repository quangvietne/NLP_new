{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9b46bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang tải dữ liệu CoNLL 2003...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b12d533a331485a8310ea3104a5c5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f0700785bf4320933b77621604a4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6d773bf2d04a758767a7f2c8c78f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b0e388b4dd4a0abf3b2a1a962c0193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a900e157d254523ad75316b889ed961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f2734a02a641e384ca68f0ced80adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các tập dữ liệu: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n",
      "\n",
      "Danh sách các nhãn (String labels): ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
      "\n",
      "--- Mẫu dữ liệu đầu tiên ---\n",
      "Tokens: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "Tags (ID): [3, 0, 7, 0, 0, 0, 7, 0, 0]\n",
      "Tags (String): ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
      "\n",
      "--- Kích thước từ điển ---\n",
      "Kích thước word_to_ix (Vocab size): 23625\n",
      "Kích thước tag_to_ix (Num labels): 9\n",
      "Danh sách tag_to_ix: {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Tải dữ liệu từ Hugging Face\n",
    "# ---------------------------------------------------------\n",
    "# Sử dụng hàm load_dataset để tải bộ dữ liệu CoNLL 2003 \n",
    "print(\"Đang tải dữ liệu CoNLL 2003...\")\n",
    "dataset = load_dataset(\"conll2003\", trust_remote_code=True)\n",
    "\n",
    "# Dữ liệu trả về là DatasetDict chứa các split: train, validation, test [cite: 29]\n",
    "print(\"Các tập dữ liệu:\", dataset)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Trích xuất câu và nhãn\n",
    "# ---------------------------------------------------------\n",
    "# Trích xuất các câu (tokens) và nhãn (ner_tags) từ tập train [cite: 32, 33]\n",
    "train_sentences = dataset[\"train\"][\"tokens\"]\n",
    "train_tags_ids = dataset[\"train\"][\"ner_tags\"]\n",
    "\n",
    "# Lấy danh sách tên nhãn (string) từ features để ánh xạ từ số sang tên [cite: 35]\n",
    "# Truy cập vào features[\"ner_tags\"].feature.names\n",
    "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(f\"\\nDanh sách các nhãn (String labels): {label_list}\")\n",
    "\n",
    "# Chuyển đổi nhãn số sang dạng string (VD: 0 -> 'O', 1 -> 'B-PER', ...) [cite: 35]\n",
    "# Bước này giúp kiểm tra dữ liệu, nhưng khi huấn luyện ta thường dùng ID.\n",
    "# Ở đây ta tạo một danh sách các nhãn dạng string để minh họa theo yêu cầu.\n",
    "train_tags_str = [[label_list[i] for i in sentence_tags] for sentence_tags in train_tags_ids]\n",
    "\n",
    "# In thử 1 mẫu dữ liệu đầu tiên\n",
    "print(\"\\n--- Mẫu dữ liệu đầu tiên ---\")\n",
    "print(\"Tokens:\", train_sentences[0])\n",
    "print(\"Tags (ID):\", train_tags_ids[0])\n",
    "print(\"Tags (String):\", train_tags_str[0])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Xây dựng Từ điển (Vocabulary)\n",
    "# ---------------------------------------------------------\n",
    "# Tạo word_to_ix: Ánh xạ mỗi từ duy nhất sang một chỉ số index [cite: 38]\n",
    "word_counts = Counter()\n",
    "for sentence in train_sentences:\n",
    "    word_counts.update(sentence)\n",
    "\n",
    "# Thêm token đặc biệt <PAD> (đệm) và <UNK> (từ lạ) \n",
    "word_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "\n",
    "# Duyệt qua các từ đã đếm được và thêm vào từ điển\n",
    "for word in word_counts:\n",
    "    if word not in word_to_ix:\n",
    "        word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "# Tạo tag_to_ix: Ánh xạ mỗi nhãn NER (dạng string) sang chỉ số nguyên \n",
    "# Ta dùng danh sách label_list đã lấy ở trên\n",
    "tag_to_ix = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Kết quả\n",
    "# ---------------------------------------------------------\n",
    "# In ra kích thước của hai từ điển [cite: 41]\n",
    "print(\"\\n--- Kích thước từ điển ---\")\n",
    "print(f\"Kích thước word_to_ix (Vocab size): {len(word_to_ix)}\")\n",
    "print(f\"Kích thước tag_to_ix (Num labels): {len(tag_to_ix)}\")\n",
    "print(f\"Danh sách tag_to_ix: {tag_to_ix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be3fae0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Kiểm tra DataLoader ---\n",
      "Shape của batch câu (Batch size, Max Seq Len): torch.Size([32, 43])\n",
      "Shape của batch nhãn: torch.Size([32, 43])\n",
      "Ví dụ câu đầu tiên trong batch (đã padding): \n",
      "tensor([2034, 7502, 7503,  131, 1825,  133, 7507, 6864,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0])\n",
      "Ví dụ nhãn đầu tiên trong batch (đã padding -1): \n",
      "tensor([ 0,  1,  2,  0,  5,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Tạo lớp NERDataset\n",
    "# ---------------------------------------------------------\n",
    "class NERDataset(Dataset):\n",
    "    # Kế thừa từ torch.utils.data.Dataset [cite: 44]\n",
    "    def __init__(self, sentences, tags, word_to_ix, tag_to_ix):\n",
    "        \"\"\"\n",
    "        Khởi tạo dataset [cite: 45]\n",
    "        :param sentences: Danh sách các câu (dạng token)\n",
    "        :param tags: Danh sách các chuỗi nhãn tương ứng (hoặc ID nhãn)\n",
    "        :param word_to_ix: Từ điển ánh xạ từ -> index\n",
    "        :param tag_to_ix: Từ điển ánh xạ nhãn -> index\n",
    "        \"\"\"\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.word_to_ix = word_to_ix\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "\n",
    "    def __len__(self):\n",
    "        # Trả về tổng số câu trong bộ dữ liệu [cite: 46]\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Lấy câu và nhãn tại vị trí index\n",
    "        tokens = self.sentences[idx]\n",
    "        label_data = self.tags[idx] # Ở Task 1 ta đã có nhãn dạng số hoặc string\n",
    "\n",
    "        # Chuyển đổi token thành index (dùng <UNK> nếu từ không có trong từ điển) \n",
    "        # word_to_ix.get(token, word_to_ix[\"<UNK>\"])\n",
    "        encoded_sents = [self.word_to_ix.get(token, self.word_to_ix[\"<UNK>\"]) for token in tokens]\n",
    "        \n",
    "        # Xử lý nhãn:\n",
    "        # Nếu label_data là list các chuỗi (như yêu cầu slide [cite: 45]), ta map qua tag_to_ix.\n",
    "        # Nếu label_data đã là list các số nguyên (từ dataset gốc), ta dùng luôn.\n",
    "        if isinstance(label_data[0], str):\n",
    "             encoded_tags = [self.tag_to_ix[tag] for tag in label_data]\n",
    "        else:\n",
    "             encoded_tags = label_data\n",
    "\n",
    "        # Trả về cặp tensor (sentence_indices, tag_indices) [cite: 47]\n",
    "        return torch.tensor(encoded_sents, dtype=torch.long), torch.tensor(encoded_tags, dtype=torch.long)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Viết hàm collate_fn để đệm (padding) dữ liệu\n",
    "# ---------------------------------------------------------\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Hàm này được gọi bởi DataLoader để gộp các mẫu đơn lẻ thành một batch.\n",
    "    Cần pad các câu về cùng độ dài của câu dài nhất trong batch[cite: 52].\n",
    "    \"\"\"\n",
    "    # Tách batch (list các tuples) thành list câu và list nhãn riêng biệt\n",
    "    sentences, tags = zip(*batch)\n",
    "    \n",
    "    # Lấy giá trị padding cho câu là index của <PAD> (thường là 0) \n",
    "    pad_val_word = word_to_ix.get(\"<PAD>\", 0)\n",
    "    \n",
    "    # Lấy giá trị padding cho nhãn (ví dụ: -1) để bỏ qua khi tính loss sau này \n",
    "    pad_val_tag = -1 \n",
    "    \n",
    "    # Sử dụng pad_sequence với batch_first=True \n",
    "    padded_sentences = pad_sequence(sentences, batch_first=True, padding_value=pad_val_word)\n",
    "    padded_tags = pad_sequence(tags, batch_first=True, padding_value=pad_val_tag)\n",
    "    \n",
    "    return padded_sentences, padded_tags\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Khởi tạo DataLoader\n",
    "# ---------------------------------------------------------\n",
    "# Chuẩn bị dữ liệu đầu vào (Lấy từ biến dataset ở Task 1)\n",
    "# Lưu ý: dataset[\"train\"][\"ner_tags\"] là số nguyên, dataset[\"train\"][\"tokens\"] là list từ.\n",
    "# Để khớp với slide yêu cầu input là \"danh sách chuỗi nhãn\"[cite: 45], \n",
    "# ta có thể dùng biến train_tags_str đã tạo ở Task 1, hoặc dùng thẳng ID gốc cho tiện.\n",
    "# Dưới đây mình dùng dữ liệu gốc từ HuggingFace cho tối ưu.\n",
    "\n",
    "train_sentences_data = dataset[\"train\"][\"tokens\"]\n",
    "train_tags_data = dataset[\"train\"][\"ner_tags\"]\n",
    "\n",
    "val_sentences_data = dataset[\"validation\"][\"tokens\"]\n",
    "val_tags_data = dataset[\"validation\"][\"ner_tags\"]\n",
    "\n",
    "# Khởi tạo dataset cho tập Train và Validation\n",
    "train_ds = NERDataset(train_sentences_data, train_tags_data, word_to_ix, tag_to_ix)\n",
    "val_ds = NERDataset(val_sentences_data, val_tags_data, word_to_ix, tag_to_ix)\n",
    "\n",
    "# Khởi tạo DataLoader [cite: 50, 51]\n",
    "BATCH_SIZE = 32 # Bạn có thể chỉnh batch size tùy ý\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,           # Train nên shuffle dữ liệu\n",
    "    collate_fn=collate_fn   # Sử dụng hàm padding tùy chỉnh [cite: 52]\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,          # Validation không cần shuffle\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Kiểm tra thử 1 batch\n",
    "# ---------------------------------------------------------\n",
    "print(\"--- Kiểm tra DataLoader ---\")\n",
    "# Lấy thử 1 batch từ train_loader\n",
    "sample_sents, sample_tags = next(iter(train_loader))\n",
    "\n",
    "print(f\"Shape của batch câu (Batch size, Max Seq Len): {sample_sents.shape}\")\n",
    "print(f\"Shape của batch nhãn: {sample_tags.shape}\")\n",
    "print(f\"Ví dụ câu đầu tiên trong batch (đã padding): \\n{sample_sents[0]}\")\n",
    "print(f\"Ví dụ nhãn đầu tiên trong batch (đã padding -1): \\n{sample_tags[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26389472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sử dụng thiết bị: cuda\n",
      "SimpleRNNForNER(\n",
      "  (embedding): Embedding(23625, 100)\n",
      "  (rnn): RNN(100, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNNForNER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size):\n",
    "        \"\"\"\n",
    "        Khởi tạo mô hình RNN cho bài toán NER.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size (int): Kích thước từ điển (số lượng từ unique).\n",
    "            embedding_dim (int): Số chiều của vector biểu diễn từ (Embedding).\n",
    "            hidden_dim (int): Số chiều của trạng thái ẩn trong RNN.\n",
    "            output_size (int): Số lượng nhãn NER (số lớp đầu ra).\n",
    "        \"\"\"\n",
    "        super(SimpleRNNForNER, self).__init__()\n",
    "        \n",
    "        # 1. nn.Embedding: Chuyển đổi chỉ số của từ thành vector \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # 2. nn.RNN: Xử lý chuỗi vector embedding [cite: 58]\n",
    "        # batch_first=True: Input shape sẽ là (batch_size, seq_len, features)\n",
    "        # Bạn có thể thay nn.RNN bằng nn.LSTM hoặc nn.GRU để mô hình học tốt hơn [cite: 58]\n",
    "        self.rnn = nn.RNN(input_size=embedding_dim, \n",
    "                          hidden_size=hidden_dim, \n",
    "                          batch_first=True)\n",
    "        \n",
    "        # 3. nn.Linear: Ánh xạ output của RNN sang không gian nhãn để dự đoán \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Lan truyền xuôi (Forward pass)\n",
    "        x shape: (batch_size, seq_len) - chứa các chỉ số từ (indices)\n",
    "        \"\"\"\n",
    "        # B1: Qua lớp Embedding\n",
    "        # shape: (batch_size, seq_len, embedding_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # B2: Qua lớp RNN\n",
    "        # rnn_out shape: (batch_size, seq_len, hidden_dim)\n",
    "        # h_n là trạng thái ẩn cuối cùng (không cần dùng ở đây)\n",
    "        rnn_out, h_n = self.rnn(embedded)\n",
    "        \n",
    "        # B3: Qua lớp Linear (Fully Connected)\n",
    "        # Áp dụng Linear cho từng token trong chuỗi\n",
    "        # logits shape: (batch_size, seq_len, output_size)\n",
    "        logits = self.fc(rnn_out)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Khởi tạo mô hình với các tham số phù hợp [cite: 62]\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Các siêu tham số (Hyperparameters)\n",
    "VOCAB_SIZE = len(word_to_ix)       # Kích thước từ điển đã tạo ở Task 1\n",
    "EMBEDDING_DIM = 100                # Chiều dài vector embedding (thường là 100, 200, 300)\n",
    "HIDDEN_DIM = 128                   # Kích thước hidden state của RNN\n",
    "OUTPUT_SIZE = len(tag_to_ix)       # Số lượng nhãn đầu ra (số class)\n",
    "\n",
    "# Kiểm tra xem có GPU không để đẩy mô hình vào\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Sử dụng thiết bị: {device}\")\n",
    "\n",
    "# Tạo instance của mô hình\n",
    "model = SimpleRNNForNER(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_SIZE)\n",
    "model = model.to(device) # Đưa mô hình vào GPU/CPU\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfe6ef5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu huấn luyện trên cuda...\n",
      "Epoch 1/5 | Loss trung bình: 0.3697\n",
      "Epoch 2/5 | Loss trung bình: 0.1077\n",
      "Epoch 3/5 | Loss trung bình: 0.0539\n",
      "Epoch 4/5 | Loss trung bình: 0.0377\n",
      "Epoch 5/5 | Loss trung bình: 0.0309\n",
      "Huấn luyện hoàn tất!\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Khởi tạo Optimizer và Loss Function\n",
    "# ---------------------------------------------------------\n",
    "# Sử dụng Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Sử dụng CrossEntropyLoss\n",
    "# Quan trọng: ignore_index=-1 để bỏ qua padding (như đã định nghĩa ở collate_fn)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Vòng lặp huấn luyện\n",
    "# ---------------------------------------------------------\n",
    "EPOCHS = 5  # Số lượng epoch (vòng lặp qua toàn bộ dữ liệu)\n",
    "\n",
    "print(f\"Bắt đầu huấn luyện trên {device}...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()  # Đặt mô hình ở chế độ huấn luyện\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (sentences, tags) in enumerate(train_loader):\n",
    "        # Chuyển dữ liệu sang thiết bị (GPU/CPU)\n",
    "        sentences = sentences.to(device)\n",
    "        tags = tags.to(device)\n",
    "        \n",
    "        # --- 5 Bước huấn luyện kinh điển ---\n",
    "        \n",
    "        # 1. Xóa gradient cũ\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward pass: Tính toán đầu ra\n",
    "        # output shape: (batch_size, seq_len, output_size)\n",
    "        predictions = model(sentences)\n",
    "        \n",
    "        # 3. Tính Loss\n",
    "        # CrossEntropyLoss yêu cầu input: (N, C) và target: (N)\n",
    "        # Ta cần làm phẳng (flatten) tensor để tính loss cho tất cả token\n",
    "        predictions = predictions.view(-1, OUTPUT_SIZE) \n",
    "        tags = tags.view(-1)\n",
    "        \n",
    "        loss = criterion(predictions, tags)\n",
    "        \n",
    "        # 4. Backward pass: Tính gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Cập nhật trọng số\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    # Tính loss trung bình của epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss trung bình: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Huấn luyện hoàn tất!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c2fd561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Độ chính xác trên tập validation: 0.9504 (95.04%)\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()  # Đặt mô hình ở chế độ đánh giá [cite: 77]\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    with torch.no_grad():  # Tắt tính toán gradient [cite: 78]\n",
    "        for sentences, tags in data_loader:\n",
    "            sentences = sentences.to(device)\n",
    "            tags = tags.to(device)\n",
    "            \n",
    "            # Lấy dự đoán từ mô hình\n",
    "            outputs = model(sentences)\n",
    "            \n",
    "            # Lấy nhãn có xác suất cao nhất (argmax) trên chiều cuối cùng [cite: 80]\n",
    "            # predictions shape: (batch_size, seq_len)\n",
    "            predictions = torch.argmax(outputs, dim=2)\n",
    "            \n",
    "            # Tính toán độ chính xác\n",
    "            # Chỉ tính trên các token không phải là padding (tag != -1) [cite: 82]\n",
    "            mask = (tags != -1)\n",
    "            \n",
    "            # So sánh dự đoán với nhãn thật tại các vị trí hợp lệ\n",
    "            correct_predictions = (predictions == tags) & mask\n",
    "            \n",
    "            total_correct += correct_predictions.sum().item()\n",
    "            total_count += mask.sum().item()\n",
    "            \n",
    "    return total_correct / total_count\n",
    "\n",
    "# Gọi hàm đánh giá và in kết quả [cite: 86]\n",
    "val_accuracy = evaluate(model, val_loader)\n",
    "print(f\"Độ chính xác trên tập validation: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab9a134c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Câu: \"VNU University is located in Hanoi\"\n",
      "Token           Nhãn dự đoán   \n",
      "------------------------------\n",
      "VNU             B-ORG          \n",
      "University      I-ORG          \n",
      "is              O              \n",
      "located         O              \n",
      "in              O              \n",
      "Hanoi           O              \n",
      "\n",
      "Câu: \"Germany 's representative to the European Union 's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .\"\n",
      "Token           Nhãn dự đoán   \n",
      "------------------------------\n",
      "Germany         B-LOC          \n",
      "'s              O              \n",
      "representative  O              \n",
      "to              O              \n",
      "the             O              \n",
      "European        B-ORG          \n",
      "Union           I-ORG          \n",
      "'s              O              \n",
      "veterinary      O              \n",
      "committee       O              \n",
      "Werner          B-PER          \n",
      "Zwingmann       I-PER          \n",
      "said            O              \n",
      "on              O              \n",
      "Wednesday       O              \n",
      "consumers       O              \n",
      "should          O              \n",
      "buy             O              \n",
      "sheepmeat       O              \n",
      "from            O              \n",
      "countries       O              \n",
      "other           O              \n",
      "than            O              \n",
      "Britain         B-LOC          \n",
      "until           O              \n",
      "the             O              \n",
      "scientific      O              \n",
      "advice          O              \n",
      "was             O              \n",
      "clearer         O              \n",
      ".               O              \n"
     ]
    }
   ],
   "source": [
    "def predict_sentence(sentence):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Tiền xử lý: Tách từ (đơn giản bằng split)\n",
    "    tokens = sentence.split()\n",
    "    \n",
    "    # 2. Chuyển từ sang index\n",
    "    # Dùng <UNK> nếu từ không có trong từ điển\n",
    "    input_ids = [word_to_ix.get(word, word_to_ix[\"<UNK>\"]) for word in tokens]\n",
    "    \n",
    "    # 3. Tạo tensor và đưa vào device (thêm chiều batch_size = 1)\n",
    "    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    \n",
    "    # 4. Dự đoán\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        # Lấy index của nhãn dự đoán\n",
    "        predicted_indices = torch.argmax(output, dim=2).squeeze().tolist()\n",
    "    \n",
    "    # 5. Ánh xạ ngược từ index sang nhãn string\n",
    "    # Chúng ta cần list nhãn để tra cứu (label_list đã lấy ở Task 1)\n",
    "    # Nếu predicted_indices là một số int (trường hợp câu 1 từ), chuyển thành list\n",
    "    if isinstance(predicted_indices, int):\n",
    "        predicted_indices = [predicted_indices]\n",
    "        \n",
    "    predicted_labels = [label_list[idx] for idx in predicted_indices]\n",
    "    \n",
    "    # 6. In kết quả\n",
    "    print(f\"\\nCâu: \\\"{sentence}\\\"\")\n",
    "    print(f\"{'Token':<15} {'Nhãn dự đoán':<15}\")\n",
    "    print(\"-\" * 30)\n",
    "    for token, label in zip(tokens, predicted_labels):\n",
    "        print(f\"{token:<15} {label:<15}\")\n",
    "\n",
    "# --- Ví dụ thực nghiệm theo yêu cầu tài liệu [cite: 96] ---\n",
    "test_sentence = \"VNU University is located in Hanoi\"\n",
    "predict_sentence(test_sentence)\n",
    "\n",
    "# Thử thêm một câu khác từ dataset mẫu\n",
    "predict_sentence(\"Germany 's representative to the European Union 's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
